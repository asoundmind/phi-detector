# Chain of Thought (CoT) Implementation Guide

This document describes the Chain of Thought reasoning enhancements implemented in the PHI Detector project.

## Overview

Chain of Thought (CoT) reasoning makes the system's decision-making process transparent and auditable by breaking down complex analysis into step-by-step reasoning chains. This is particularly valuable for privacy compliance where understanding the "why" behind recommendations is critical.

## What Was Implemented

### 1. Core CoT Framework (`src/chain_of_thought.py`)

**ReasoningChain Class**: Manages a chain of thought reasoning process
- Stores multiple reasoning steps
- Formats output for display
- Supports JSON serialization for logging/auditing

**ReasoningStep Class**: Represents individual steps in reasoning
- Step description
- Findings/observations
- Conclusions

**Specialized Reasoning Modules**:
- **ClassificationReasoning**: Transparent message classification (PII ticket, dev ticket, or question)
- **RiskAssessmentReasoning**: Step-by-step risk level determination
- **ComplianceReasoning**: Multi-step RAG analysis for comprehensive policy retrieval

### 2. Enhanced Prompts (`src/prompt_templates.py`)

Added CoT-enhanced prompts that explicitly guide the LLM through step-by-step reasoning:

- **COT_DETECTION_PROMPT**: 5-step analysis for PII tickets
  - Understanding the data
  - Regulatory analysis
  - Risk evaluation
  - Compliance requirements
  - Recommended actions

- **COT_POLICY_QUESTION_PROMPT**: 5-step analysis for policy questions
  - Understanding the question
  - Analyzing policy context
  - Identifying requirements
  - Practical application
  - Final answer with citations

- **COT_DEV_TICKET_PROMPT**: 6-step analysis for development tickets
  - Understanding technical requirements
  - Classification and applicability
  - Analyzing regulatory requirements
  - Technical security measures
  - Implementation guidance
  - Risk assessment and best practices

### 3. Chatbot Integration (`src/chatbot.py`)

Updated all analysis methods to support CoT reasoning:

**Classification with Reasoning**:
```python
msg_type, classification_reasoning = chatbot._classify_message(
    message,
    return_reasoning=True
)
```

**Risk Assessment with Reasoning**:
```python
risk_level, risk_reasoning = chatbot._assess_risk_level(
    detections,
    return_reasoning=True
)
```

**Multi-Step RAG Analysis**:
```python
rag_reasoning, enhanced_context = ComplianceReasoning.multi_step_analysis(
    ticket_text,
    initial_context,
    rag_system
)
```

### 4. UI Integration (`app.py`)

Added expandable sections in Streamlit UI to display reasoning:

- **üß† Classification Reasoning**: Shows how the message was classified
- **‚ö†Ô∏è Risk Assessment Reasoning**: Displays step-by-step risk analysis
- **üîç Multi-Step RAG Analysis**: Shows how policy documents were retrieved
- All reasoning is collapsible to avoid cluttering the interface

## How It Works

### Example 1: Message Classification

**Input**: "As a PM, I'm building a chat system. Does it need encryption?"

**Reasoning Output**:
```
Step 1: Scanning for actual PII patterns
  - ‚úó Email pattern not found
  - ‚úó Phone pattern not found
  - ‚úó SSN pattern not found
  ‚Üí Conclusion: No PII patterns ‚Üí Not a PII ticket

Step 2: Checking for development/PM indicators
  - Found: 'pm'
  - Found: 'building'
  - Found: 'chat system'
  - Found: 'encryption'
  - Total: 4 development indicators
  ‚Üí Conclusion: 4 indicators present

Step 3: Analyzing question patterns
  - ‚úì Contains question words (does, need)
  ‚Üí Conclusion: Message is asking questions

Final Conclusion: Development/Requirements Ticket
Confidence: HIGH
```

### Example 2: Risk Assessment

**Input**: Ticket with SSN, Email, and Phone Number

**Reasoning Output**:
```
Step 1: Cataloging detected PII types
  - SSN: 1 instance(s) [CRITICAL category]
  - EMAIL: 1 instance(s) [MEDIUM category]
  - PHONE_NUMBER: 1 instance(s) [MEDIUM category]
  ‚Üí Conclusion: Found 3 PII items across 3 types

Step 2: Identifying highest risk elements
  - ‚ö†Ô∏è CRITICAL types found: SSN
  - Triggers automatic CRITICAL classification
  ‚Üí Conclusion: Risk level determined: CRITICAL

Step 3: Considering context and linkability
  - Multiple identifier types (3) increase linkability risk
  ‚Üí Conclusion: Context analysis complete

Final Conclusion: CRITICAL
Primary Factor: Presence of SSN
Secondary Factors: High linkability (multiple identifier types)
Confidence: HIGH
```

### Example 3: Multi-Step RAG Analysis

**Input**: "We're building a patient portal. What security measures do we need?"

**Reasoning Output**:
```
Step 1: Initial broad policy search
  - Retrieved 3 relevant policy chunks
  - Sources: PHIPA.txt, PIPEDA.txt
  ‚Üí Conclusion: Found 3 relevant passages

Step 2: Identifying gaps and performing follow-up queries
  - Identified specific topics: encryption standards, patient portal requirements
  - ‚úì Follow-up query 'encryption standards': 2 results
  - ‚úì Follow-up query 'patient portal requirements': 2 results
  ‚Üí Conclusion: Total context expanded to 7 passages

Step 3: Deduplicating context
  - Total passages retrieved: 7
  - Duplicates removed: 1
  - Unique passages: 6
  ‚Üí Conclusion: Final context: 6 unique passages

Final Conclusion: Multi-step analysis complete with 6 unique policy references
Confidence: HIGH
```

## Benefits

### 1. Transparency
Users can see exactly how decisions were made, building trust in the system.

### 2. Auditability
Reasoning chains can be logged and reviewed for compliance audits.

### 3. Education
Users learn about privacy compliance through the step-by-step explanations.

### 4. Debugging
When the system makes an unexpected classification, the reasoning shows why.

### 5. Better Answers
Multi-step RAG retrieves more comprehensive and relevant policy context.

### 6. Confidence Assessment
Each reasoning chain includes a confidence level.

## Usage

### Basic Usage (CoT Enabled by Default)

```python
from src.chatbot import ChatBot

chatbot = ChatBot()

# CoT is enabled by default
response, prompt, metadata, msg_type = chatbot.chat(
    "User test@test.com (SSN: 123-45-6789) reports issue",
    return_prompt=True
)

# Access reasoning chains from metadata
classification_reasoning = metadata.get('classification_reasoning')
risk_reasoning = metadata.get('risk_reasoning')
rag_reasoning = metadata.get('rag_reasoning')

# Display reasoning
if classification_reasoning:
    print(classification_reasoning.format())
```

### Disabling CoT (for faster responses)

```python
# Disable CoT for faster processing
response = chatbot.chat(
    "What are consent requirements?",
    use_cot=False
)
```

### Using Individual Reasoning Modules

```python
from src.chain_of_thought import ClassificationReasoning, RiskAssessmentReasoning

# Classification reasoning
patterns = {'Email': True, 'Phone': True, 'SSN': False}
chain = ClassificationReasoning.analyze_pii_patterns(message, patterns)
print(chain.format())

# Risk assessment reasoning
detections = [{'type': 'SSN', 'value': '123-45-6789'}]
chain = RiskAssessmentReasoning.assess_risk(
    detections,
    critical_types={'SSN'},
    high_risk_types={'CREDIT_CARD'},
    medium_risk_types={'EMAIL'}
)
print(chain.format())
```

### Creating Custom Reasoning Chains

```python
from src.chain_of_thought import ReasoningChain

chain = ReasoningChain("Custom Policy Analysis")

chain.add_step(
    "Analyzing context",
    ["Finding 1", "Finding 2", "Finding 3"],
    "Conclusion from this step"
)

chain.add_step(
    "Determining requirements",
    ["Requirement 1", "Requirement 2"],
    "Final requirement determination"
)

chain.set_conclusion("Overall conclusion", "HIGH")

print(chain.format())

# Export to JSON for logging
import json
print(json.dumps(chain.to_dict(), indent=2))
```

## UI Features

### In Streamlit App

When you run `streamlit run app.py`, you'll see:

1. **Classification Badge**: Shows message type (üé´ PII, üõ†Ô∏è Dev, ‚ùì Question)

2. **Expandable Reasoning Sections**:
   - üß† Classification Reasoning
   - ‚ö†Ô∏è Risk Assessment Reasoning
   - üîç Multi-Step RAG Analysis

3. **Prompt Viewer**: See the exact prompt sent to the LLM

4. **Policy References**: View source documents with relevance scores

5. **Chat History**: Past queries include their reasoning chains

## Testing

Run the CoT test suite:

```bash
python test_cot.py
```

This tests:
- Classification reasoning
- Risk assessment reasoning
- Custom reasoning chains
- JSON serialization

## Performance Considerations

### Token Usage
CoT prompts are longer and generate more detailed responses:
- Standard prompt: ~600 tokens
- CoT prompt: ~800 tokens
- Impact: 30-50% more tokens per request

### Response Time
- Multi-step RAG adds 1-2 seconds for follow-up queries
- Overall impact: ~20-40% slower responses
- Trade-off: Much more comprehensive and accurate answers

### Recommendations
- Use CoT for:
  - Complex compliance questions
  - Development guidance
  - Risk assessments
  - Audit trails

- Disable CoT for:
  - Simple lookups
  - Performance-critical applications
  - High-volume automated processing

## Architecture Decisions

### Why Hybrid Approach?
We implemented a hybrid system where CoT can be toggled:
- Users can disable for speed when needed
- Critical paths (risk assessment, classification) always generate reasoning
- Reasoning display is optional (collapsible in UI)

### Why Separate Reasoning Modules?
- **Modularity**: Each reasoning type can be tested independently
- **Reusability**: Reasoning chains can be used outside the chatbot
- **Maintainability**: Easy to update or add new reasoning types

### Why Store Reasoning in Metadata?
- **Flexibility**: UI can choose how to display it
- **Backward Compatibility**: Existing code works without reasoning
- **Logging**: Reasoning can be logged for audits without changing main flow

## Future Enhancements

Potential areas for expansion:

1. **Self-Verification**: LLM checks its own answers against sources
2. **Tree of Thoughts**: Explore multiple reasoning branches
3. **Confidence Scores**: More granular confidence assessment
4. **Learning from Feedback**: Track when reasoning led to good/bad outcomes
5. **Reasoning Templates**: Pre-built templates for common scenarios
6. **Visualization**: Graph-based reasoning chain display

## Files Modified/Added

### New Files
- `src/chain_of_thought.py` - Core CoT framework
- `test_cot.py` - Test suite
- `COT_IMPLEMENTATION.md` - This document

### Modified Files
- `src/chatbot.py` - Added CoT support to all methods
- `src/prompt_templates.py` - Added CoT-enhanced prompts
- `app.py` - Added reasoning display in UI

## Examples in the Wild

Try these examples in the Streamlit app to see CoT in action:

1. **PII Ticket**:
   ```
   User john@test.com (SIN: 123-456-789) reports issue at 555-1234
   ```
   ‚Üí See classification + risk assessment reasoning

2. **Dev Ticket**:
   ```
   As a PM, I'm building a patient messaging system.
   What encryption is required for compliance?
   ```
   ‚Üí See classification + multi-step RAG reasoning

3. **Policy Question**:
   ```
   What are the consent requirements under PIPEDA?
   ```
   ‚Üí See classification + step-by-step answer construction

## Troubleshooting

**Issue**: Reasoning chains not appearing in UI
- **Solution**: Ensure `use_cot=True` in chatbot.chat() call
- **Check**: Look for metadata keys in response

**Issue**: CoT responses are too slow
- **Solution**: Set `use_cot=False` for faster responses
- **Alternative**: Reduce `top_k` parameter to retrieve fewer documents

**Issue**: Reasoning formatting looks wrong
- **Solution**: Use `.format(include_header=False)` in UI
- **Check**: Ensure using `st.text()` not `st.markdown()` for formatting

## Questions?

For questions or issues with CoT implementation:
1. Check the test suite: `python test_cot.py`
2. Review reasoning chain output format
3. Verify metadata contains reasoning objects
4. Check that CoT is enabled in chatbot calls

---

**Implementation Date**: December 2024
**Version**: 1.0
**Author**: Claude Code CoT Enhancement
